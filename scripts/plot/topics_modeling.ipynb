{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46f9b334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß† TF-IDF + NMF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "import glob, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebd7714f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Th√®me 1 :\n",
      "min | cannes | festival | film | cin√©ma | genre | croisette | r√©alisateur | acteur | comp√©tition\n",
      "\n",
      "\n",
      "Th√®me 2 :\n",
      "candidat | droite | sondage | extr√™me | √©lection | pologne | scrutin | tour | sortie | urne\n",
      "\n",
      "\n",
      "Th√®me 3 :\n",
      "drone | fibre | filaire | optique | russe | ukraine | anton | ukrainien | appareil | vite\n",
      "\n",
      "\n",
      "Th√®me 4 :\n",
      "isra√©lien | gaza | isra√´l | espoir | bande | hamas | offensive | palestinien | tr√™ve | humanitaire\n",
      "\n",
      "\n",
      "Th√®me 5 :\n",
      "nestl√© | eau | rapport | sant√© | alexandre | sanitaire | enqu√™te | waters | agence | ouizill\n",
      "\n",
      "\n",
      "Th√®me 6 :\n",
      "vol | a√©rien | passager | compagnie | heure | paris | retard | dimanche | europ√©en | consommateur\n",
      "\n",
      "\n",
      "Th√®me 7 :\n",
      "bruno | retailleau | wauquiez | lr | laurent | parti | d√©put√© | r√©publicains | √©lu | r√©publicain\n",
      "\n",
      "\n",
      "Th√®me 8 :\n",
      "personne | mourir | rue | collectif | mort | eau | chiffre | euthanasie | rapport | d√©c√©der\n",
      "\n",
      "\n",
      "Th√®me 9 :\n",
      "malade | euro | million | si√®ge | s√©curit√© | milliard | emploi | patient | paris | r√©duire\n",
      "\n",
      "\n",
      "Th√®me 10 :\n",
      "ufc | association | choisir | attendre | plainte | victime | affaire | h√¥pital | marque | explosion\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleaned_docs = \"../../data/clean\"\n",
    "\n",
    "# Charger tous les fichiers .txt dans une liste\n",
    "file_paths = glob.glob(os.path.join(cleaned_docs, \"*.txt\"))\n",
    "documents = []\n",
    "for path in file_paths:\n",
    "    with open(path, encoding='utf-8') as f:\n",
    "        documents.append(f.read())\n",
    "\n",
    "# Vectorisation TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.9, min_df=2)\n",
    "tfidf = tfidf_vectorizer.fit_transform(documents)\n",
    "\n",
    "# Mod√©lisation de NMF\n",
    "nmf = NMF(n_components=10, random_state=42)\n",
    "nmf_topics = nmf.fit_transform(tfidf)\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Affichage des th√®mes\n",
    "def display_and_save_topics(model, feature_names, n_top_words, output_file):\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for i, topic in enumerate(model.components_):\n",
    "            top_words = \" | \".join([feature_names[j] for j in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "            output = f\"Th√®me {i+1} :\\n{top_words}\\n\\n\"\n",
    "            print(output)\n",
    "            f.write(output)\n",
    "\n",
    "# Appel de la fonction\n",
    "output_path = \"../../figures/nmf_topics.txt\"\n",
    "display_and_save_topics(nmf, feature_names, 10, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d6e684-472a-4f1a-ad71-afe438f3f13f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
